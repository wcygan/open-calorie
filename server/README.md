# Server

Responsible for triggering inference on an LLM and returning the result to the client.